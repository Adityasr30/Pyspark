{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5836913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "# Author: Group1 \n",
    "# Created: 10th May 2023\n",
    "# Last Modified: 10th May 2023\n",
    "\n",
    "# Description: This pyspark application is converting json data into csv format\n",
    "# Reference - Project SRS doc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # defining SparkSession object\n",
    "    spark = SparkSession.builder.master(\"yarn\").appName(\"Json-to-CSV\").getOrCreate()\n",
    "    sc=spark.sparkContext\n",
    "    #Definning Logger\n",
    "    log4jLogger = sc._jvm.org.apache.log4j\n",
    "    logger = log4jLogger.LogManager.getLogger(\"jsontocsv\")\n",
    "    appID = spark.sparkContext.applicationId\n",
    "\n",
    "    PROJECT_PATH = \"/home/talentum/zomato_etl\"\n",
    "    try:\n",
    "        # Creating a DataFrame\n",
    "        logger.info(f'-----appID is : {appID}')\n",
    "\n",
    "        # Set the input and output directories\n",
    "        logger.warn(\"Input and Output Directory Set\")\n",
    "        input_dir = PROJECT_PATH+\"/source/json\"\n",
    "        output_dir = \"file://\"+PROJECT_PATH+\"/source/csv\"\n",
    "\n",
    "        # listing all the json files in the given directory\n",
    "        json_files = [file for file in os.listdir(input_dir) if file.endswith('.json')]\n",
    "        \n",
    "        # removing existing csv files\n",
    "        rm_cmd=PROJECT_PATH+\"/source/csv/*\"\n",
    "        os.system(f\"rm -rf {rm_cmd}\")\n",
    "\n",
    "        logger.info(f'-----json file to loaded to spark from path: {input_dir}')\n",
    "\n",
    "        # iterating through all json files in the list and selecting only required columns\n",
    "        for i, file in enumerate(sorted(json_files)):\n",
    "            logger.warn(f\"{file} Read\")\n",
    "            df=spark.read.json(\"file://\"+input_dir+\"/\"+file)\n",
    "            res_df=df.select(F.explode(df.restaurants.restaurant).alias(\"res\"))\n",
    "            logger.info('-----Exploded column restaurants.restaurant')\n",
    "            # selecting required columns\n",
    "            data=res_df.select(\n",
    "                \"res.R.res_id\",\n",
    "                    \"res.name\",\n",
    "                    \"res.location.country_id\",\n",
    "                    \"res.location.city\",\n",
    "                    \"res.location.address\",\n",
    "                    \"res.location.locality\",\n",
    "                    \"res.location.locality_verbose\",\n",
    "                    \"res.location.longitude\",\n",
    "                    \"res.location.latitude\",\n",
    "                    \"res.cuisines\",\n",
    "                    \"res.average_cost_for_two\",\n",
    "                    \"res.currency\",\n",
    "                    \"res.has_table_booking\",\n",
    "                    \"res.has_online_delivery\",\n",
    "                    \"res.is_delivering_now\",\n",
    "                    \"res.switch_to_order_menu\",\n",
    "                    \"res.price_range\",\n",
    "                    \"res.user_rating.aggregate_rating\",\n",
    "                    \"res.user_rating.rating_text\",\n",
    "                    \"res.user_rating.votes\"\n",
    "                )\n",
    "            logger.info('-----Selected required columns')\n",
    "            #defining header\n",
    "            headers = ['Restaurant ID', 'Restaurant Name', 'Country Code', 'City', 'Address', 'Locality', 'Locality Verbose', 'Longitude',\n",
    "           'Latitude', 'Cuisines', 'Average Cost for two', 'Currency', 'Has Table booking', 'Has Online delivery', 'Is delivering now',\n",
    "           'Switch to order menu', 'Price range', 'Aggregate rating', 'Rating text', 'Votes']\n",
    "            # adding header to the df\n",
    "            data = data.toDF(*headers)\n",
    "            logger.info('-----Header added to DataFrame')\n",
    "            logger.info('---------------------------Printing SCHEMA----------------------------')\n",
    "            logger.info(res_df.printSchema())\n",
    "            data.write.mode(\"append\").option(\"delimiter\",\"\\t\").csv(f\"{output_dir}/zomato_csv\")\n",
    "            logger.warn(f\"{file} Wrote\")\n",
    "        spark.stop()\n",
    "\n",
    "\n",
    "    except  IOError as e:\n",
    "        print(\"**********************************************************************\")\n",
    "        print(\"\\033[91mHad an IOException trying to read that file\\033[0m\")\n",
    "        print(\"**********************************************************************\")\n",
    "\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"**********************************************************************\")\n",
    "        print(\"\\033[91mCaught File Not Found Exception!\\033[0m\")\n",
    "        print(\"**********************************************************************\")\n",
    "\n",
    "\n",
    "    except IndexError as e:\n",
    "        print(\"**********************************************************************\")\n",
    "        print(\"\\033[91mCaught Array Index Out Of Bounds Exception, Kindly Input Parameters On Invoking This Script\\033[0m\")\n",
    "        print(\"**********************************************************************\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"**********************************************************************\")\n",
    "        print(\"\\033[91mCaught an Error, Kindly Refer Logs. Failed Status Updated!\\033[0m\")\n",
    "        print(\"**********************************************************************\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
